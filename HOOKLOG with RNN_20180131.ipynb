{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets('C:/test/tensorflow/mnist/input_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unrolled through 28 time steps\n",
    "time_steps = 8\n",
    "#hidden LSTM units\n",
    "num_units = 128\n",
    "#rows of 28 pixels\n",
    "n_input = 8\n",
    "#learning rate for adam\n",
    "learning_rate = 0.001\n",
    "#mnist is meant to be classified in 10 classes(0-9).\n",
    "#n_classes=10\n",
    "#size of batch\n",
    "batch_size = 16\n",
    "\n",
    "test_rate = 0.1\n",
    "h_size = time_steps * n_input\n",
    "API_padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../MotifAnalysis/Hooklog3.ipynb\n",
    "Hooklog = Hooklog3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old data\n",
    "'''\n",
    "in_base_dir = \"C:/test/tensorflow/hooklog/\"\n",
    "in_directories = [\"allaple_woj_g_98_year2017\", # 16k\n",
    "                  \"somoto_woj_year2017\", # 17k\n",
    "                  \"morstar_g_54\", # 85k\n",
    "                  \"bettersurf_woj_g_137+\", # 106k\n",
    "                  \"elkern_woj_g_127\", # 44k\n",
    "                  \"lydra_woj_g_44\", # 30k\n",
    "                  \"mira_woj_g_107\", # 14k\n",
    "                  \"sytro_woj_g_166\" # 34k\n",
    "                 ]\n",
    "\n",
    "# new data\n",
    "in_base_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/hooklogs/\"\n",
    "in_directories = [\n",
    "    \"allaple_woj_g_98_year2017\",\n",
    "    \"bettersurf_woj_g_137+\",\n",
    "    \"elkern_woj_g_127\",\n",
    "    \"fakeav_g_132\",\n",
    "    \"loring_g_15\",\n",
    "    \"morstar_g_54\",\n",
    "    \"rahack_g_39\",\n",
    "    \"sytro_woj_g_166\"\n",
    "]\n",
    "'''\n",
    "\n",
    "# yj\n",
    "in_base_dir = \"C:/test/hooklog_family_yj/hooklog/\"\n",
    "in_directories = None\n",
    "if in_directories is None:\n",
    "    import os\n",
    "    in_directories = next(os.walk(in_base_dir))[1]\n",
    "\n",
    "# real dirs\n",
    "in_directories = [in_base_dir + d + '/' for d in in_directories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 ['10.downloa', '13.strictor', '16.vobfus', '17.mplug', '19.solimba', '21.autoit', '23.expiro', '27.allaple', '35.rahack']\n"
     ]
    }
   ],
   "source": [
    "in_parseFirstPar = False\n",
    "\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Dropbox/notebook/MotifAnalysis/pickle/\"\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/pickles/\"\n",
    "in_apifreq_dir = \"C:/test/hooklog_family_yj/pickle/\"\n",
    "\n",
    "in_apifreq_dicts = [in_apifreq_dir+\"apifreq_dict_\"+d.split(\"/\")[-2]+\".pickle\" for d in in_directories]\n",
    "\n",
    "n_classes = len(in_directories)\n",
    "classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "print(n_classes, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TerminateProcess': ('TerminateProcess', 0, 0.0, 8, 0.0001410288051334485), 'GetUrlCacheEntryInfo': ('GetUrlCacheEntryInfo', 1, 0.058823529411764705, 29, 0.0005112294186087508), 'CopyFile': ('CopyFile', 2, 0.11764705882352941, 51, 0.0008990586327257342), 'InternetConnect': ('InternetConnect', 3, 0.17647058823529413, 54, 0.0009519444346507774), 'HttpSendRequest': ('HttpSendRequest', 4, 0.23529411764705882, 54, 0.0009519444346507774), 'InternetOpen': ('InternetOpen', 5, 0.29411764705882354, 57, 0.0010048302365758205), 'ExitProcess': ('ExitProcess', 6, 0.35294117647058826, 65, 0.0011458590417092692), 'CreateProcess': ('CreateProcess', 7, 0.4117647058823529, 151, 0.0026619186968938405), 'CreateThread': ('CreateThread', 8, 0.47058823529411764, 152, 0.002679547297535522), 'RegDeleteKey': ('RegDeleteKey', 9, 0.5294117647058824, 175, 0.003085005112294186), 'CreateProcessInternal': ('CreateProcessInternal', 10, 0.5882352941176471, 299, 0.005270951591862638), 'DeleteFile': ('DeleteFile', 11, 0.6470588235294118, 896, 0.015795226174946234), 'RegSetValue': ('RegSetValue', 12, 0.7058823529411765, 1505, 0.02653104396573), 'RegCreateKey': ('RegCreateKey', 13, 0.7647058823529411, 1659, 0.029245848464548883), 'LoadLibrary': ('LoadLibrary', 14, 0.8235294117647058, 5460, 0.0962521595035786), 'RegEnumValue': ('RegEnumValue', 15, 0.8823529411764706, 6481, 0.11425096075873498), 'CreateFile': ('CreateFile', 16, 0.9411764705882353, 10390, 0.18316116066706625), 'RegQueryValue': ('RegQueryValue', 17, 1.0, 29240, 0.5154602827627542)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "apifreq_dict = dict()\n",
    "_total = 0\n",
    "for pkf in in_apifreq_dicts:\n",
    "    with open(pkf, 'rb') as f:\n",
    "        this_dict = pickle.load(f)\n",
    "        for k in this_dict:\n",
    "            if k in apifreq_dict:\n",
    "                apifreq_dict[k] += this_dict[k]\n",
    "            else:\n",
    "                apifreq_dict[k] = this_dict[k]\n",
    "            _total += this_dict[k]\n",
    "\n",
    "s_dict = {item[0]: item for item in [(k, i, i/(len(apifreq_dict)-1), apifreq_dict[k], apifreq_dict[k]/_total) for i, k in enumerate(sorted(apifreq_dict, key = apifreq_dict.get, reverse=False))]}\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "xx_train_list = list()\n",
    "yy_train_list = list()\n",
    "\n",
    "xx_test_list = list()\n",
    "yy_test_list = list()\n",
    "\n",
    "train_name_list = list()\n",
    "test_name_list = list()\n",
    "\n",
    "name_dict = dict()\n",
    "\n",
    "for label, in_dir in enumerate(in_directories):\n",
    "    hl_list = next(os.walk(in_dir))[2] # get all filenames in the in_directory\n",
    "    hl_list = [os.path.join(in_dir, f) for f in hl_list] # filepathname list\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".hooklog\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    \n",
    "    #shuffle list\n",
    "    random.shuffle(hl_list) # <-------------- random here\n",
    "    test_size = int(len(hl_list)*test_rate)\n",
    "\n",
    "    for i, file in enumerate(hl_list):\n",
    "        hl3 = Hooklog(file, in_parseFirstPar)\n",
    "        li_li = list() # for hacking moving_window # 20171129\n",
    "        \n",
    "        for start in range(0, len(hl3.li), h_size):\n",
    "            end = start + h_size\n",
    "            #print(hl3.digitname, start, end)\n",
    "            \n",
    "            li = list()\n",
    "            for (t, api) in hl3.li[start:end]:\n",
    "                li.append(s_dict[api][4]) # <-- encode\n",
    "            # hack\n",
    "            if len(li) < h_size:\n",
    "                #print(\"!!! change img_cols to a smaller number\", len(hl3.li))\n",
    "                #print(hl3.digitname, \"has smaller size\", len(hl3.li), \"need img_cols\", str(img_cols))\n",
    "                if API_padding:\n",
    "                    #print(\"padding 1.0\", str(img_cols - len(hl3.li)))\n",
    "                    for _ in range(h_size - len(li)):\n",
    "                        li.append(1.0)\n",
    "            li_li.append(li)\n",
    "\n",
    "            if(i < test_size):\n",
    "                xx_test_list.extend([li])\n",
    "                a = [0] * n_classes\n",
    "                a[label] = 1\n",
    "                yy_test_list.extend([a])\n",
    "                test_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "            else:\n",
    "                xx_train_list.extend([li])\n",
    "                a = [0] * n_classes\n",
    "                a[label] = 1\n",
    "                yy_train_list.extend([a])\n",
    "                train_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "                \n",
    "            break; # stop moving window!! Must break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173, 64)\n",
      "(173, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x_train = np.ndarray(shape = (len(xx_train_list), img_rows, img_cols), buffer = np.array(xx_train_list))\n",
    "#y_train = np.array(yy_train_list)\n",
    "x_train = np.ndarray(shape = (len(xx_train_list), time_steps * n_input), buffer = np.array(xx_train_list))\n",
    "y_train = np.array(yy_train_list)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 64)\n",
      "(15, 9)\n"
     ]
    }
   ],
   "source": [
    "#x_test = x_train.copy()\n",
    "#y_test = y_train.copy()\n",
    "\n",
    "x_test= np.ndarray(shape = (len(xx_test_list), time_steps * n_input), buffer = np.array(xx_test_list))\n",
    "y_test = np.array(yy_test_list)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input image placeholder\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input_x=tf.unstack(x, time_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the network\n",
    "with tf.variable_scope('anystring', reuse = tf.AUTO_REUSE):\n",
    "    lstm_layer=tf.contrib.rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "    outputs,_=tf.nn.static_rnn(lstm_layer,input_x,dtype=\"float32\") # Mike: I change it to tf.nn.\n",
    "    \n",
    "    #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "    prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "    \n",
    "    \n",
    "    #loss_function\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    #optimization\n",
    "    opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    #model evaluation\n",
    "    correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_next_batch(x_test, y_test, seq, start, batch_size):\n",
    "    end = start + batch_size\n",
    "    if end > len(x_test):\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(len(x_test))\n",
    "        np.random.shuffle(perm)\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        seq = perm\n",
    "    return x_test[seq][start:end], y_test[seq][start:end], seq, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch: 10 Accuracy: 0.0 Loss: 4.57935\n",
      "For epoch: 20 Accuracy: 0.6875 Loss: 2.01232\n",
      "For epoch: 30 Accuracy: 0.125 Loss: 2.05776\n",
      "For epoch: 40 Accuracy: 0.3125 Loss: 2.04054\n",
      "For epoch: 50 Accuracy: 0.25 Loss: 1.91506\n",
      "For epoch: 60 Accuracy: 0.625 Loss: 1.53506\n",
      "For epoch: 70 Accuracy: 0.625 Loss: 1.37525\n",
      "For epoch: 80 Accuracy: 0.625 Loss: 1.27842\n",
      "For epoch: 90 Accuracy: 0.5625 Loss: 1.20963\n",
      "For epoch: 100 Accuracy: 0.6875 Loss: 0.917984\n",
      "For epoch: 110 Accuracy: 0.6875 Loss: 0.712812\n",
      "For epoch: 120 Accuracy: 0.6875 Loss: 0.90244\n",
      "For epoch: 130 Accuracy: 0.875 Loss: 0.544405\n",
      "For epoch: 140 Accuracy: 0.8125 Loss: 0.583215\n",
      "For epoch: 150 Accuracy: 0.8125 Loss: 0.491002\n",
      "For epoch: 160 Accuracy: 0.875 Loss: 0.462655\n",
      "For epoch: 170 Accuracy: 0.875 Loss: 0.464364\n",
      "For epoch: 180 Accuracy: 0.875 Loss: 0.389735\n",
      "For epoch: 190 Accuracy: 0.8125 Loss: 0.524883\n",
      "For epoch: 200 Accuracy: 0.875 Loss: 0.368602\n",
      "For epoch: 210 Accuracy: 0.75 Loss: 0.625343\n",
      "For epoch: 220 Accuracy: 0.8125 Loss: 0.458657\n",
      "For epoch: 230 Accuracy: 0.8125 Loss: 0.445227\n",
      "For epoch: 240 Accuracy: 0.9375 Loss: 0.281536\n",
      "For epoch: 250 Accuracy: 0.875 Loss: 0.361729\n",
      "For epoch: 260 Accuracy: 0.8125 Loss: 0.32336\n",
      "For epoch: 270 Accuracy: 0.875 Loss: 0.512817\n",
      "For epoch: 280 Accuracy: 0.8125 Loss: 0.392784\n",
      "For epoch: 290 Accuracy: 0.8125 Loss: 0.458226\n",
      "For epoch: 300 Accuracy: 0.8125 Loss: 0.517479\n",
      "For epoch: 310 Accuracy: 0.75 Loss: 0.456831\n",
      "For epoch: 320 Accuracy: 0.9375 Loss: 0.253199\n",
      "For epoch: 330 Accuracy: 0.75 Loss: 0.686395\n",
      "For epoch: 340 Accuracy: 0.8125 Loss: 0.342618\n",
      "For epoch: 350 Accuracy: 0.875 Loss: 0.276631\n",
      "For epoch: 360 Accuracy: 0.875 Loss: 0.282702\n",
      "For epoch: 370 Accuracy: 0.5 Loss: 0.954717\n",
      "For epoch: 380 Accuracy: 0.6875 Loss: 0.648237\n",
      "For epoch: 390 Accuracy: 0.75 Loss: 0.440288\n",
      "For epoch: 400 Accuracy: 0.8125 Loss: 0.419381\n",
      "For epoch: 410 Accuracy: 0.8125 Loss: 0.354134\n",
      "For epoch: 420 Accuracy: 0.6875 Loss: 0.571731\n",
      "For epoch: 430 Accuracy: 0.8125 Loss: 0.359166\n",
      "For epoch: 440 Accuracy: 0.8125 Loss: 0.597032\n",
      "For epoch: 450 Accuracy: 0.8125 Loss: 0.301324\n",
      "For epoch: 460 Accuracy: 1.0 Loss: 0.182404\n",
      "For epoch: 470 Accuracy: 0.9375 Loss: 0.343029\n",
      "For epoch: 480 Accuracy: 0.9375 Loss: 0.285513\n",
      "For epoch: 490 Accuracy: 1.0 Loss: 0.214876\n"
     ]
    }
   ],
   "source": [
    "#initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "epoch = 1\n",
    "\n",
    "b = 0\n",
    "seq = np.arange(len(x_train))\n",
    "\n",
    "while epoch < 500:\n",
    "    #batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n",
    "    batch_x, batch_y, seq, b = my_next_batch(x_train, y_train, seq, b, batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size, time_steps, n_input))\n",
    "    #print(type(batch_x), batch_x.shape, type(batch_y), batch_y.shape)\n",
    "\n",
    "    sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "    if epoch %10 == 0:\n",
    "        acc = sess.run(accuracy, feed_dict = {x: batch_x, y: batch_y})\n",
    "        los = sess.run(loss, feed_dict = {x: batch_x, y: batch_y})\n",
    "        print(\"For epoch:\", epoch, \"Accuracy:\", acc, \"Loss:\",los)\n",
    "        #if acc >= 0.95: break # hack\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.933333\n"
     ]
    }
   ],
   "source": [
    "#calculating test accuracy\n",
    "#test_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))\n",
    "#test_label = mnist.test.labels[:128]\n",
    "test_data = x_test.reshape((-1, time_steps, n_input))\n",
    "test_label = y_test\n",
    "print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict = {x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 train_able_variables in the Graph\n",
      "<tf.Variable 'Variable:0' shape=(128, 9) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(9,) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(136, 512) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'forward/rnn/basic_lstm_cell/kernel:0' shape=(136, 512) dtype=float32_ref>\n",
      "<tf.Variable 'forward/rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'anystring/rnn/basic_lstm_cell/kernel:0' shape=(136, 512) dtype=float32_ref>\n",
      "<tf.Variable 'anystring/rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_2:0' shape=(128, 9) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_3:0' shape=(9,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_4:0' shape=(128, 9) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_5:0' shape=(9,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_6:0' shape=(128, 9) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_7:0' shape=(9,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_8:0' shape=(128, 9) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_9:0' shape=(9,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_10:0' shape=(128, 9) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_11:0' shape=(9,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_12:0' shape=(128, 9) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_13:0' shape=(9,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "vs = tf.trainable_variables()\n",
    "print('There are', len(vs), 'train_able_variables in the Graph')\n",
    "for v in vs:\n",
    "    print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-cpu]",
   "language": "python",
   "name": "conda-env-tensorflow-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
